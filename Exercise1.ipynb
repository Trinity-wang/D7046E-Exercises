{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Learning Machines\n",
    "## Exercise 1 - Getting started with Jupyter notebooks and Pytorch\n",
    "In this exercise you will design and train simple neural network to classify digits from the MNIST (http://yann.lecun.com/exdb/mnist/) dataset, like those in the image below.\n",
    "\n",
    "![MNIST digits](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "The purpose of the exercise is to familiarise yourself with Jupyter notebooks and Pytorch. Don't worry if you do not understand the details behind this code. You will understand most of it after completing the course.\n",
    "\n",
    "Before you start this exercise you need to have properly installed a conda environment according to the PDF that accompany the exercise. If you have done this properly, the code below should run without error.\n",
    "\n",
    "Press the CONTROL + ENTER keys to run a selected block of code or text. Double click text to edit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Yes!? Python works alright :-)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MNIST dataset\n",
    "In order to train a neural network we need some data to train the network with.\n",
    "For example, we can use the MNIST dataset of handwritten digits to train a network.\n",
    "MNIST consist of 70,000 grayscale images of size 28x28 pixels each.\n",
    "\n",
    "Our goal here is to train a network that can recognize what digit an image represents.\n",
    "\n",
    "The code below gets the dataset (downloads it if necessary) and displays one of the images.\n",
    "\n",
    "**Exercise:** What do the digits on positions 2 and 9 have in common? Can you find another similar digit? \n",
    "The upper part of the 2 and 9 can both have an upper round part and a line down. Depending on how you write it there can be a horizontal part.  \n",
    "1 and 7 are similar if you write the 1 with a line a the top. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = datasets.MNIST('./', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "mnist_loader = DataLoader(mnist_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "def plot_digit(data):\n",
    "    # Transfrom the images into an appropriate shape for displaying\n",
    "    data = data.view(28,28)\n",
    "    plt.imshow(data, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "images, labels = next(iter(mnist_loader))\n",
    "plot_digit(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch\n",
    "\n",
    "In this course we will be using the deep learning framework **PyTorch**.\n",
    "This exercise will be a light introduction to the framework, but to really get to know PyTorch we recommend this tutorial [https://pytorch.org/tutorials/beginner/pytorch_with_examples.html ] or finding another tutorial that suits you better.\n",
    "\n",
    "\n",
    "To solve this task you will need to improve the neural network provided below.\n",
    "This can be done by adding or changing the modules that are part of nn.Sequential. A list of available modules can be found here: [https://pytorch.org/docs/stable/nn.html] though it's recommended that you stick with nn.Linear, nn.Sigmoid, nn.ReLU, nn.LeakyReLU, or similar modules. Feed-Forward networks (the type recommended for this task, we'll tell you more about that later) usually consist of nn.Linear separated by activation functions (like nn.Sigmoid, and nn.Linear). Each nn.Linear layer must have the input size (first parameter) of the previous layer's output size. E.g. nn.Linear(784, 50) could be followed by nn.Linear(50, 10). \n",
    "\n",
    "For inspiration, see the MNIST website [http://yann.lecun.com/exdb/mnist/ ] which contains some previously tested network architectures and the corresponding classification accuracy obtained.\n",
    "\n",
    "Additionally, you might want to change the optimizer used; (List here: [https://pytorch.org/docs/stable/optim.html ])\n",
    "or loss function (List here: [https://pytorch.org/docs/stable/nn.html#loss-functions ])\n",
    "\n",
    "Note: Not all loss functions takes the same input so you might need to restructure the data to make it work.\n",
    "\n",
    "Any network that you design will be randomly initialized and therefore bad at recognizing images initially, which you will experience when running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code initializes the neural network\n",
    "\n",
    "### EDIT CODE BELOW TO CHANGE THE NETWORK AND ITS OPTIMIZING PROCEDURE ###\n",
    "# nn.Sequential can be given a list of neural networks modules\n",
    "\n",
    "# This initial network has only one single linear layer.\n",
    "# It has an input size equal to the size of the images (28x28 pixels = 784)\n",
    "# and an output size equal to the number of classes (the number of digits = 10)\n",
    "network = nn.Sequential(\n",
    "    nn.Linear(784, 784),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(784, 784),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(784, 784),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(784, 10)\n",
    ")\n",
    "\n",
    "# Initialize the optimizer\n",
    "# In addition to changing optimizer you can try to change other parameters like learning rate (lr)\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.15)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "### EDIT CODE ABOVE TO CHANGE THE NETWORK AND ITS OPTIMIZING PROCEDURE ###\n",
    "\n",
    "# An Embedding layer used for turning int into one-hot (0 -> [1,0,0,0,0,0,0,0,0,0], 5 -> [0,0,0,0,0,1,0,0,0,0])\n",
    "to_onehot = nn.Embedding(10, 10) \n",
    "to_onehot.weight.data = torch.eye(10)\n",
    "\n",
    "# Extract some images from the dataset and have the new predict what digit they are\n",
    "def predict_on_images(iterator, images_to_show):\n",
    "    success=0\n",
    "    for index in range(images_to_show):\n",
    "        # Get the next batch of images\n",
    "        images, labels = next(iterator)\n",
    "\n",
    "        #plot_digit(images[0])\n",
    "\n",
    "        # Transform the images into a single list of pixels since our network takes that as its input\n",
    "        input_tensor = images[0].view(1,784)\n",
    "        # Run the input through our network to get a prediction\n",
    "        prediction = network(input_tensor)\n",
    "        # Extract which prediction had the highest probability\n",
    "        guess = torch.argmax(prediction[0], dim=-1)\n",
    "        # Show the predicted digit and the actual digit\n",
    "        print('Prediction:', guess.item(), \"- Actual:\", labels[0].item())\n",
    "        if guess.item()==labels[0].item():\n",
    "            success+=1\n",
    "    print(\"Success rate is \", success, \" on \", images_to_show)\n",
    "\n",
    "# Have the untrained network predict on some images\n",
    "predict_on_images(iterator = iter(mnist_loader), images_to_show = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "Below is the training procedure. Neural networks are usually trained with backpropagation which works as follows (for now you don't need to understand the prodcedure, but you'll be expected to learn it in the course later on).\n",
    "    1. Give the network the input and have it calculate a prediction.\n",
    "    2. Calculate the loss/error by comparing the difference between the prediction and the target output.\n",
    "    3. For the error E and each parameter w find their error gradient: (backpropagate the error)\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial E}{\\partial w}\n",
    "\\end{equation*}\n",
    "    4. Update the parameters according to their error gradient (since we know how this parameter affected the error we can change it to cause less error)\n",
    "    5. Repeat from step 1 with new input\n",
    "    \n",
    "The code below **takes a while to execute**. Note the cirle at the top-right corner of the notebook, just next to the name Python followed by the version number, like **Python 3**. When the circle is filled the code is still running. An open circle means that code is not running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide the number of epochs to train for (one epoch is one optimization iteration on the entire dataset)\n",
    "epochs = 30\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # For each batch\n",
    "    for batch_nr, (images, labels) in enumerate(mnist_loader):\n",
    "        \n",
    "        # Extract the labels and turn them into one-hot representation (note: not all loss functions needs this)\n",
    "        labels = to_onehot(labels)\n",
    "        \n",
    "        # Reshape the images to a single vector (28*28 = 784)\n",
    "        images = images.view(-1,784)\n",
    "        \n",
    "        # Predict for each digit in the batch whatclass they belong to\n",
    "        prediction = network(images)\n",
    "        \n",
    "        # Calculate the loss of the prediction by comparing to the expected output\n",
    "        loss = loss_function(prediction, labels)\n",
    "        \n",
    "        # Backpropagate the loss through the network to find the gradients of all parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters along their gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Clear stored gradient values\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Print the epoch, batch, and loss\n",
    "        print(\n",
    "            '\\rEpoch {} [{}/{}] - Loss: {}'.format(\n",
    "                epoch+1, batch_nr+1, len(mnist_loader), loss\n",
    "            ),\n",
    "            end=''\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the network\n",
    "To see whether our network have been trained properly we want to test it on new data, which has not been used during training. In Exercise 2 you'll need to implement a proper testing procedure, but for now we simplify things and just predict a number of digits to see whether it looks fairly alright.\n",
    "\n",
    "**Exercise:** Can you improve the network architecture to improve the classification results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Have the trained network predict on a number of images\n",
    "predict_on_images(iterator = iter(mnist_loader), images_to_show = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
